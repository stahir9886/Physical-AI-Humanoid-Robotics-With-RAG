"use strict";(globalThis.webpackChunktextbook_website=globalThis.webpackChunktextbook_website||[]).push([[96],{85:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>m,frontMatter:()=>o,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"chapter-6-capstone/index","title":"Capstone - Building a Complete Physical AI System","description":"Introduction","source":"@site/docs/chapter-6-capstone/index.md","sourceDirName":"chapter-6-capstone","slug":"/chapter-6-capstone/","permalink":"/docs/chapter-6-capstone/","draft":false,"unlisted":false,"editUrl":"https://github.com/your-username/Physical-AI-Humanoid-Robotics/edit/main/docusaurus/docs/chapter-6-capstone/index.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"sidebar_position":6,"title":"Capstone - Building a Complete Physical AI System"},"sidebar":"tutorialSidebar","previous":{"title":"Vision-Language-Action Systems","permalink":"/docs/chapter-5-vision-language-action-systems/"}}');var s=t(4848),a=t(8453);const o={sidebar_position:6,title:"Capstone - Building a Complete Physical AI System"},r="Capstone - Building a Complete Physical AI System",l={},c=[{value:"Introduction",id:"introduction",level:2},{value:"Project Overview: Interactive Humanoid Assistant",id:"project-overview-interactive-humanoid-assistant",level:2},{value:"System Requirements",id:"system-requirements",level:3},{value:"Functional Requirements",id:"functional-requirements",level:4},{value:"Non-Functional Requirements",id:"non-functional-requirements",level:4},{value:"System Architecture",id:"system-architecture",level:2},{value:"High-Level Architecture",id:"high-level-architecture",level:3},{value:"Component Breakdown",id:"component-breakdown",level:3},{value:"Web Interface Layer",id:"web-interface-layer",level:4},{value:"AI Service Layer",id:"ai-service-layer",level:4},{value:"Robot Control Layer",id:"robot-control-layer",level:4},{value:"Simulation Layer",id:"simulation-layer",level:4},{value:"Implementation Plan",id:"implementation-plan",level:2},{value:"Phase 1: Infrastructure Setup",id:"phase-1-infrastructure-setup",level:3},{value:"1.1 ROS 2 Workspace Creation",id:"11-ros-2-workspace-creation",level:4},{value:"1.2 Package Structure",id:"12-package-structure",level:4},{value:"1.3 Basic Node Implementation",id:"13-basic-node-implementation",level:4},{value:"Phase 2: Vision-Language Integration",id:"phase-2-vision-language-integration",level:3},{value:"2.1 Vision Processing Node",id:"21-vision-processing-node",level:4},{value:"2.2 Natural Language Processing Module",id:"22-natural-language-processing-module",level:4},{value:"Phase 3: Simulation Environment",id:"phase-3-simulation-environment",level:3},{value:"3.1 Gazebo World Setup",id:"31-gazebo-world-setup",level:4},{value:"3.2 Robot Model Configuration",id:"32-robot-model-configuration",level:4},{value:"Phase 4: AI Service Integration",id:"phase-4-ai-service-integration",level:3},{value:"4.1 RAG Backend Implementation",id:"41-rag-backend-implementation",level:4},{value:"Phase 5: Frontend Integration",id:"phase-5-frontend-integration",level:3},{value:"5.1 Chatbot Component",id:"51-chatbot-component",level:4},{value:"Testing the Complete System",id:"testing-the-complete-system",level:2},{value:"1. Local Development Setup",id:"1-local-development-setup",level:3},{value:"2. Integration Testing",id:"2-integration-testing",level:3},{value:"2.1 Textbook Content Testing",id:"21-textbook-content-testing",level:4},{value:"2.2 RAG System Testing",id:"22-rag-system-testing",level:4},{value:"2.3 Simulation Integration Testing",id:"23-simulation-integration-testing",level:4},{value:"Deployment Considerations",id:"deployment-considerations",level:2},{value:"1. Frontend Deployment (Vercel)",id:"1-frontend-deployment-vercel",level:3},{value:"2. Backend Deployment (Railway/Render)",id:"2-backend-deployment-railwayrender",level:3},{value:"3. Environment Configuration",id:"3-environment-configuration",level:3},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"1. Frontend Optimizations",id:"1-frontend-optimizations",level:3},{value:"2. Backend Optimizations",id:"2-backend-optimizations",level:3},{value:"3. Simulation Optimizations",id:"3-simulation-optimizations",level:3},{value:"Security Considerations",id:"security-considerations",level:2},{value:"1. API Security",id:"1-api-security",level:3},{value:"2. Data Security",id:"2-data-security",level:3},{value:"Future Enhancements",id:"future-enhancements",level:2},{value:"1. Advanced AI Capabilities",id:"1-advanced-ai-capabilities",level:3},{value:"2. Extended Simulation Environments",id:"2-extended-simulation-environments",level:3},{value:"3. Enhanced User Interfaces",id:"3-enhanced-user-interfaces",level:3},{value:"Conclusion",id:"conclusion",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(e.header,{children:(0,s.jsx)(e.h1,{id:"capstone---building-a-complete-physical-ai-system",children:"Capstone - Building a Complete Physical AI System"})}),"\n",(0,s.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsx)(e.p,{children:"In this capstone chapter, we will synthesize all the concepts covered throughout this textbook to design, implement, and deploy a complete Physical AI system. This project will integrate:"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Physical AI principles learned in Chapter 1"}),"\n",(0,s.jsx)(e.li,{children:"Humanoid robotics concepts from Chapter 2"}),"\n",(0,s.jsx)(e.li,{children:"ROS 2 infrastructure from Chapter 3"}),"\n",(0,s.jsx)(e.li,{children:"Simulation capabilities from Chapter 4"}),"\n",(0,s.jsx)(e.li,{children:"Vision-Language-Action systems from Chapter 5"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:"Our goal is to create a humanoid robot system that can understand natural language commands, perceive its environment, and execute appropriate actions in a simulated environment."}),"\n",(0,s.jsx)(e.h2,{id:"project-overview-interactive-humanoid-assistant",children:"Project Overview: Interactive Humanoid Assistant"}),"\n",(0,s.jsx)(e.p,{children:"We will build an interactive humanoid assistant capable of:"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:"Understanding natural language commands"}),"\n",(0,s.jsx)(e.li,{children:"Perceiving its environment through vision"}),"\n",(0,s.jsx)(e.li,{children:"Planning and executing tasks in a 3D environment"}),"\n",(0,s.jsx)(e.li,{children:"Interacting safely with objects and humans"}),"\n",(0,s.jsx)(e.li,{children:"Learning from interactions to improve performance"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"system-requirements",children:"System Requirements"}),"\n",(0,s.jsx)(e.h4,{id:"functional-requirements",children:"Functional Requirements"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"FR-001"}),": The system shall accept natural language instructions"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"FR-002"}),": The system shall perceive and understand its 3D environment"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"FR-003"}),": The system shall plan and execute robot actions to accomplish tasks"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"FR-004"}),": The system shall operate safely in human environments"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"FR-005"}),": The system shall learn from interactions to improve over time"]}),"\n"]}),"\n",(0,s.jsx)(e.h4,{id:"non-functional-requirements",children:"Non-Functional Requirements"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"NFR-001"}),": The system shall respond to commands within 5 seconds"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"NFR-002"}),": The system shall maintain 95% task completion rate"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"NFR-003"}),": The system shall operate within free-tier cloud resource limits"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"NFR-004"}),": The system shall be deployable as a web-based interface"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"system-architecture",children:"System Architecture"}),"\n",(0,s.jsx)(e.h3,{id:"high-level-architecture",children:"High-Level Architecture"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{children:"\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   User Input    \u2502\u2500\u2500\u2500\u25b6\u2502   AI Service     \u2502\u2500\u2500\u2500\u25b6\u2502  Robot Control  \u2502\n\u2502   (Web UI)      \u2502    \u2502   (RAG Backend)  \u2502    \u2502   (ROS 2 Nodes) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                                \u25bc\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502  Simulation      \u2502\n                    \u2502  Environment     \u2502\n                    \u2502  (Gazebo/Isaac)  \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"})}),"\n",(0,s.jsx)(e.h3,{id:"component-breakdown",children:"Component Breakdown"}),"\n",(0,s.jsx)(e.h4,{id:"web-interface-layer",children:"Web Interface Layer"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Frontend"}),": Docusaurus-based interface with embedded chatbot"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"User Interaction"}),": Natural language input and response display"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Visualization"}),": 3D rendering of robot environment"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"API Communication"}),": Integration with backend services"]}),"\n"]}),"\n",(0,s.jsx)(e.h4,{id:"ai-service-layer",children:"AI Service Layer"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Language Understanding"}),": Process natural language commands"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"RAG System"}),": Retrieve relevant information from textbook content"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Task Planning"}),": Decompose high-level commands into executable actions"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Vision Processing"}),": Interpret visual input from simulation"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Action Selection"}),": Choose appropriate robot actions based on state"]}),"\n"]}),"\n",(0,s.jsx)(e.h4,{id:"robot-control-layer",children:"Robot Control Layer"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"ROS 2 Nodes"}),": Individual nodes for different robot functions"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Navigation"}),": Path planning and obstacle avoidance"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Manipulation"}),": Object grasping and manipulation"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Perception"}),": Processing sensor data from simulation"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Behavior Trees"}),": Structured execution of robot behaviors"]}),"\n"]}),"\n",(0,s.jsx)(e.h4,{id:"simulation-layer",children:"Simulation Layer"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Environment"}),": 3D simulation with realistic physics"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Robot Model"}),": Humanoid robot model with proper kinematics"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Objects"}),": Interactive objects in the environment"]}),"\n",(0,s.jsxs)(e.li,{children:[(0,s.jsx)(e.strong,{children:"Sensors"}),": Simulated cameras, IMU, and other sensors"]}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"implementation-plan",children:"Implementation Plan"}),"\n",(0,s.jsx)(e.h3,{id:"phase-1-infrastructure-setup",children:"Phase 1: Infrastructure Setup"}),"\n",(0,s.jsx)(e.h4,{id:"11-ros-2-workspace-creation",children:"1.1 ROS 2 Workspace Creation"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"# Create the workspace\nmkdir -p ~/physical_ai_ws/src\ncd ~/physical_ai_ws\n\n# Create the robot control package\ncd src\nros2 pkg create --build-type ament_python robot_control\n"})}),"\n",(0,s.jsx)(e.h4,{id:"12-package-structure",children:"1.2 Package Structure"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{children:"robot_control/\n\u251c\u2500\u2500 robot_control/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 navigation/\n\u2502   \u2502   \u251c\u2500\u2500 navigator.py\n\u2502   \u2502   \u2514\u2500\u2500 path_planner.py\n\u2502   \u251c\u2500\u2500 manipulation/\n\u2502   \u2502   \u251c\u2500\u2500 gripper_control.py\n\u2502   \u2502   \u2514\u2500\u2500 grasp_planner.py\n\u2502   \u251c\u2500\u2500 perception/\n\u2502   \u2502   \u251c\u2500\u2500 object_detector.py\n\u2502   \u2502   \u2514\u2500\u2500 scene_analyzer.py\n\u2502   \u2514\u2500\u2500 behavior/\n\u2502       \u251c\u2500\u2500 behavior_tree.py\n\u2502       \u2514\u2500\u2500 state_machine.py\n\u251c\u2500\u2500 launch/\n\u2502   \u2514\u2500\u2500 robot_system.launch.py\n\u251c\u2500\u2500 config/\n\u2502   \u2514\u2500\u2500 params.yaml\n\u2514\u2500\u2500 test/\n    \u2514\u2500\u2500 test_robot_control.py\n"})}),"\n",(0,s.jsx)(e.h4,{id:"13-basic-node-implementation",children:"1.3 Basic Node Implementation"}),"\n",(0,s.jsx)(e.p,{children:"Create a basic robot command node:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"# robot_control/robot_control/command_node.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Pose\nfrom robot_control_interfaces.msg import RobotCommand  # Custom message\n\nclass RobotCommandNode(Node):\n    def __init__(self):\n        super().__init__('robot_command_node')\n        \n        # Create publisher for robot commands\n        self.command_publisher = self.create_publisher(\n            RobotCommand, 'robot_command', 10\n        )\n        \n        # Create subscription for high-level commands\n        self.command_subscriber = self.create_subscription(\n            String, 'high_level_command', self.command_callback, 10\n        )\n        \n        self.get_logger().info('Robot Command Node initialized')\n    \n    def command_callback(self, msg):\n        # Process the high-level command and convert to robot actions\n        self.get_logger().info(f'Received command: {msg.data}')\n        \n        # This is where we would integrate with the AI service\n        # For now, we'll create a simple command\n        command = RobotCommand()\n        command.action = 'move_to_location'\n        command.target_location = Pose()  # Set appropriate values\n        command.description = msg.data\n        \n        self.command_publisher.publish(command)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = RobotCommandNode()\n    \n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(e.h3,{id:"phase-2-vision-language-integration",children:"Phase 2: Vision-Language Integration"}),"\n",(0,s.jsx)(e.h4,{id:"21-vision-processing-node",children:"2.1 Vision Processing Node"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:"# robot_control/robot_control/perception/object_detector.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom vision_msgs.msg import Detection2DArray\nimport cv2\nfrom cv_bridge import CvBridge\nimport numpy as np\n\nclass ObjectDetector(Node):\n    def __init__(self):\n        super().__init__('object_detector')\n        \n        self.bridge = CvBridge()\n        \n        # Subscriptions\n        self.image_sub = self.create_subscription(\n            Image, '/camera/rgb/image_raw', self.image_callback, 10\n        )\n        self.camera_info_sub = self.create_subscription(\n            CameraInfo, '/camera/rgb/camera_info', self.camera_info_callback, 10\n        )\n        \n        # Publishers\n        self.detection_pub = self.create_publisher(\n            Detection2DArray, '/object_detections', 10\n        )\n        \n        self.camera_info = None\n        \n    def camera_info_callback(self, msg):\n        self.camera_info = msg\n    \n    def image_callback(self, msg):\n        # Convert ROS image to OpenCV\n        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')\n        \n        # Perform object detection\n        detections = self.detect_objects(cv_image)\n        \n        # Publish detections\n        detection_msg = Detection2DArray()\n        detection_msg.header = msg.header\n        detection_msg.detections = detections\n        \n        self.detection_pub.publish(detection_msg)\n    \n    def detect_objects(self, image):\n        # Use a pre-trained model or implement object detection\n        # For this example, we'll use a simple method\n        # In practice, you might use YOLO, SSD, or other approaches\n        pass\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = ObjectDetector()\n    \n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n"})}),"\n",(0,s.jsx)(e.h4,{id:"22-natural-language-processing-module",children:"2.2 Natural Language Processing Module"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'# In the AI service layer\n# ai_service/nlp_processor.py\nimport openai\nfrom typing import Dict, List, Tuple\nfrom dataclasses import dataclass\n\n@dataclass\nclass ParsedCommand:\n    action: str\n    target: str\n    location: str\n    properties: Dict[str, str]\n\nclass NLPProcessor:\n    def __init__(self, api_key: str):\n        self.client = openai.OpenAI(api_key=api_key)\n    \n    def parse_command(self, command: str) -> ParsedCommand:\n        """\n        Parse a natural language command into structured action\n        """\n        system_prompt = """\n        You are a robot command parser. Parse human language commands into structured actions.\n        The output should be in the format:\n        ACTION: [action to perform]\n        TARGET: [object to interact with]\n        LOCATION: [where to go or where the object is]\n        PROPERTIES: [any specific properties mentioned]\n        \n        Example:\n        Input: "Can you bring me the red cup from the kitchen table?"\n        Output:\n        ACTION: fetch_object\n        TARGET: red cup\n        LOCATION: kitchen table\n        PROPERTIES: red\n        """\n        \n        response = self.client.chat.completions.create(\n            model="gpt-3.5-turbo",\n            messages=[\n                {"role": "system", "content": system_prompt},\n                {"role": "user", "content": command}\n            ],\n            temperature=0.1\n        )\n        \n        parsed_response = self._parse_llm_response(response.choices[0].message.content)\n        return parsed_response\n    \n    def _parse_llm_response(self, response: str) -> ParsedCommand:\n        lines = response.strip().split(\'\\n\')\n        action = ""\n        target = ""\n        location = ""\n        properties = {}\n        \n        for line in lines:\n            if line.startswith("ACTION:"):\n                action = line.split(":", 1)[1].strip()\n            elif line.startswith("TARGET:"):\n                target = line.split(":", 1)[1].strip()\n            elif line.startswith("LOCATION:"):\n                location = line.split(":", 1)[1].strip()\n            elif line.startswith("PROPERTIES:"):\n                prop_str = line.split(":", 1)[1].strip()\n                if prop_str:\n                    properties = {"description": prop_str}\n        \n        return ParsedCommand(action, target, location, properties)\n    \n    def ground_command(self, command: str, scene_description: str):\n        """\n        Ground the command in the current scene context\n        """\n        system_prompt = f"""\n        Given the current scene:\n        {scene_description}\n        \n        And the user command: "{command}"\n        \n        Ground the command to specific objects in the scene.\n        Identify which objects in the scene match the command targets.\n        """\n        \n        response = self.client.chat.completions.create(\n            model="gpt-3.5-turbo",\n            messages=[\n                {"role": "system", "content": system_prompt},\n                {"role": "user", "content": f"Command: {command}"}\n            ],\n            temperature=0.1\n        )\n        \n        return response.choices[0].message.content\n'})}),"\n",(0,s.jsx)(e.h3,{id:"phase-3-simulation-environment",children:"Phase 3: Simulation Environment"}),"\n",(0,s.jsx)(e.h4,{id:"31-gazebo-world-setup",children:"3.1 Gazebo World Setup"}),"\n",(0,s.jsx)(e.p,{children:"Create a world file for our simulation:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-xml",children:'\x3c!-- worlds/physical_ai_lab.sdf --\x3e\n<sdf version="1.6">\n  <world name="physical_ai_lab">\n    \x3c!-- Include a ground plane --\x3e\n    <include>\n      <uri>model://ground_plane</uri>\n    </include>\n    \n    \x3c!-- Include lighting --\x3e\n    <include>\n      <uri>model://sun</uri>\n    </include>\n    \n    \x3c!-- Lab environment --\x3e\n    <model name="lab_room">\n      <pose>0 0 0 0 0 0</pose>\n      <static>true</static>\n      <link name="room_structure">\n        <visual name="room_visual">\n          <geometry>\n            <box>\n              <size>10 10 3</size>\n            </box>\n          </geometry>\n          <material>\n            <ambient>0.9 0.9 0.9 1</ambient>\n            <diffuse>0.9 0.9 0.9 1</diffuse>\n          </material>\n        </visual>\n        <collision name="room_collision">\n          <geometry>\n            <box>\n              <size>10 10 3</size>\n            </box>\n          </geometry>\n        </collision>\n      </link>\n    </model>\n    \n    \x3c!-- Furniture --\x3e\n    <model name="table">\n      <pose>2 0 0 0 0 0</pose>\n      <include>\n        <uri>model://table</uri>\n      </include>\n    </model>\n    \n    \x3c!-- Objects for interaction --\x3e\n    <model name="red_cup">\n      <pose>2.1 0.1 0.8 0 0 0</pose>\n      <link name="cup_link">\n        <visual name="cup_visual">\n          <geometry>\n            <cylinder>\n              <radius>0.05</radius>\n              <length>0.1</length>\n            </cylinder>\n          </geometry>\n          <material>\n            <ambient>0.8 0.2 0.2 1</ambient>\n            <diffuse>0.8 0.2 0.2 1</diffuse>\n          </material>\n        </visual>\n        <collision name="cup_collision">\n          <geometry>\n            <cylinder>\n              <radius>0.05</radius>\n              <length>0.1</length>\n            </cylinder>\n          </geometry>\n        </collision>\n        <inertial>\n          <mass>0.1</mass>\n          <inertia>\n            <ixx>0.001</ixx>\n            <iyy>0.001</iyy>\n            <izz>0.001</izz>\n          </inertia>\n        </inertial>\n      </link>\n    </model>\n    \n    \x3c!-- Humanoid robot --\x3e\n    <model name="humanoid_robot">\n      <pose>-1 0 0.8 0 0 0</pose>\n      <include>\n        <uri>model://humanoid_robot</uri>\n      </include>\n    </model>\n  </world>\n</sdf>\n'})}),"\n",(0,s.jsx)(e.h4,{id:"32-robot-model-configuration",children:"3.2 Robot Model Configuration"}),"\n",(0,s.jsx)(e.p,{children:"Create a basic humanoid robot model (URDF/SDF):"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-xml",children:'\x3c!-- models/humanoid_robot/model.sdf --\x3e\n<?xml version="1.0" ?>\n<sdf version="1.6">\n  <model name="humanoid_robot">\n    \x3c!-- Main body --\x3e\n    <link name="base_link">\n      <pose>0 0 1.0 0 0 0</pose>\n      <inertial>\n        <mass>20.0</mass>\n        <inertia>\n          <ixx>1.0</ixx>\n          <iyy>1.0</iyy>\n          <izz>1.0</izz>\n          <ixy>0.0</ixy>\n          <ixz>0.0</ixz>\n          <iyz>0.0</iyz>\n        </inertia>\n      </inertial>\n      <visual name="base_visual">\n        <geometry>\n          <box>\n            <size>0.3 0.3 0.5</size>\n          </box>\n        </geometry>\n      </visual>\n      <collision name="base_collision">\n        <geometry>\n          <box>\n            <size>0.3 0.3 0.5</size>\n          </box>\n        </geometry>\n      </collision>\n    </link>\n    \n    \x3c!-- Head --\x3e\n    <link name="head">\n      <pose>0 0 0.3 0 0 0</pose>\n      <inertial>\n        <mass>2.0</mass>\n        <inertia>\n          <ixx>0.1</ixx>\n          <iyy>0.1</iyy>\n          <izz>0.1</izz>\n          <ixy>0.0</ixy>\n          <ixz>0.0</ixz>\n          <iyz>0.0</iyz>\n        </inertia>\n      </inertial>\n      <visual name="head_visual">\n        <geometry>\n          <sphere>\n            <radius>0.15</radius>\n          </sphere>\n        </geometry>\n      </visual>\n      <collision name="head_collision">\n        <geometry>\n          <sphere>\n            <radius>0.15</radius>\n          </sphere>\n        </geometry>\n      </collision>\n    </link>\n    \n    <joint name="neck_joint" type="revolute">\n      <parent>base_link</parent>\n      <child>head</child>\n      <axis>\n        <xyz>0 1 0</xyz>\n        <limit>\n          <lower>-0.5</lower>\n          <upper>0.5</upper>\n          <effort>10.0</effort>\n          <velocity>1.0</velocity>\n        </limit>\n      </axis>\n    </joint>\n    \n    \x3c!-- Sensors --\x3e\n    <sensor name="camera" type="camera">\n      <pose>0.05 0 0.2 0 0 0</pose>\n      <camera name="head_camera">\n        <horizontal_fov>1.047</horizontal_fov>\n        <image>\n          <width>640</width>\n          <height>480</height>\n        </image>\n        <clip>\n          <near>0.1</near>\n          <far>10</far>\n        </clip>\n      </camera>\n    </sensor>\n  </model>\n</sdf>\n'})}),"\n",(0,s.jsx)(e.h3,{id:"phase-4-ai-service-integration",children:"Phase 4: AI Service Integration"}),"\n",(0,s.jsx)(e.h4,{id:"41-rag-backend-implementation",children:"4.1 RAG Backend Implementation"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-python",children:'# backend/src/api/main.py\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom typing import List, Dict, Any\nimport uvicorn\nimport asyncio\nfrom qdrant_client import QdrantClient\nfrom qdrant_client.http.models import PointStruct, VectorParams, Distance\nimport openai\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.vectorstores import Qdrant\n\napp = FastAPI(title="Physical AI & Humanoid Robotics RAG API")\n\n# Configuration\nQDRANT_URL = "http://localhost:6333"  # or your Qdrant instance\nOPENAI_API_KEY = "your-openai-api-key"  # In production, use environment variables\n\n# Initialize clients\nqdrant_client = QdrantClient(url=QDRANT_URL)\nopenai.api_key = OPENAI_API_KEY\n\n# Initialize vector store\nembeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n\n# Define data models\nclass ChatQuery(BaseModel):\n    query: str\n    context: Dict[str, Any] = {}\n\nclass ChatResponse(BaseModel):\n    response: str\n    sources: List[str]\n    confidence: float\n\nclass TextbookChapter(BaseModel):\n    id: str\n    title: str\n    content: str\n    metadata: Dict[str, Any] = {}\n\n@app.on_event("startup")\nasync def startup_event():\n    # Initialize the vector store if not exists\n    try:\n        # Create collection if it doesn\'t exist\n        qdrant_client.create_collection(\n            collection_name="textbook_content",\n            vectors_config=VectorParams(size=1536, distance=Distance.COSINE)\n        )\n    except Exception as e:\n        # Collection might already exist, which is fine\n        pass\n\n@app.post("/api/chat/query", response_model=ChatResponse)\nasync def chat_query(query: ChatQuery):\n    """\n    Process a natural language query against the textbook content\n    """\n    try:\n        # Search for relevant content in the vector store\n        vector_store = Qdrant(\n            client=qdrant_client,\n            collection_name="textbook_content",\n            embeddings=embeddings\n        )\n        \n        # Perform similarity search\n        docs = vector_store.similarity_search(query.query, k=5)\n        \n        # Prepare context for the LLM\n        context_text = "\\n".join([doc.page_content for doc in docs])\n        \n        # Use OpenAI to generate a response based on the context\n        system_prompt = f"""\n        You are an expert AI assistant for a Physical AI & Humanoid Robotics textbook. \n        Use the following context to answer the user\'s question:\n        \n        {context_text}\n        \n        If the context doesn\'t contain the information needed to answer the question, \n        respond that you don\'t have sufficient information from the textbook.\n        """\n        \n        response = openai.ChatCompletion.create(\n            model="gpt-3.5-turbo",\n            messages=[\n                {"role": "system", "content": system_prompt},\n                {"role": "user", "content": query.query}\n            ],\n            temperature=0.3\n        )\n        \n        return ChatResponse(\n            response=response.choices[0].message.content,\n            sources=[doc.metadata.get("source", "unknown") for doc in docs],\n            confidence=0.8  # In a real implementation, calculate actual confidence score\n        )\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n@app.get("/api/chapters")\nasync def get_chapters():\n    """\n    Get a list of all available textbook chapters\n    """\n    # In a real implementation, this would fetch from a database\n    chapters = [\n        {"id": "1", "title": "Introduction to Physical AI"},\n        {"id": "2", "title": "Basics of Humanoid Robotics"},\n        {"id": "3", "title": "ROS 2 Fundamentals"},\n        {"id": "4", "title": "Digital Twin Simulation (Gazebo + Isaac)"},\n        {"id": "5", "title": "Vision-Language-Action Systems"},\n        {"id": "6", "title": "Capstone - Building a Complete Physical AI System"}\n    ]\n    return chapters\n\n@app.get("/api/chapters/{chapter_id}")\nasync def get_chapter(chapter_id: str):\n    """\n    Get content of a specific textbook chapter\n    """\n    # This would typically fetch from a database in a real implementation\n    chapters_map = {\n        "1": "Introduction to Physical AI",\n        "2": "Basics of Humanoid Robotics", \n        "3": "ROS 2 Fundamentals",\n        "4": "Digital Twin Simulation (Gazebo + Isaac)",\n        "5": "Vision-Language-Action Systems",\n        "6": "Capstone - Building a Complete Physical AI System"\n    }\n    \n    if chapter_id not in chapters_map:\n        raise HTTPException(status_code=404, detail="Chapter not found")\n    \n    # In a real implementation, this would return the actual chapter content\n    return {\n        "id": chapter_id,\n        "title": chapters_map[chapter_id],\n        "content": f"Content for chapter {chapters_map[chapter_id]}"\n    }\n\nif __name__ == "__main__":\n    uvicorn.run(app, host="0.0.0.0", port=8000)\n'})}),"\n",(0,s.jsx)(e.h3,{id:"phase-5-frontend-integration",children:"Phase 5: Frontend Integration"}),"\n",(0,s.jsx)(e.h4,{id:"51-chatbot-component",children:"5.1 Chatbot Component"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-tsx",children:"// docusaurus/src/components/Chatbot/index.tsx\nimport React, { useState, useEffect, useRef } from 'react';\nimport BrowserOnly from '@docusaurus/BrowserOnly';\n\ninterface Message {\n  id: string;\n  content: string;\n  role: 'user' | 'assistant';\n  timestamp: Date;\n}\n\nconst Chatbot: React.FC = () => {\n  const [messages, setMessages] = useState<Message[]>([\n    { \n      id: '1', \n      content: 'Hello! I am your Physical AI & Humanoid Robotics assistant. Ask me anything about the textbook content.', \n      role: 'assistant', \n      timestamp: new Date() \n    }\n  ]);\n  const [inputValue, setInputValue] = useState('');\n  const [isLoading, setIsLoading] = useState(false);\n  const messagesEndRef = useRef<null | HTMLDivElement>(null);\n\n  const scrollToBottom = () => {\n    messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' });\n  };\n\n  useEffect(() => {\n    scrollToBottom();\n  }, [messages]);\n\n  const handleSubmit = async (e: React.FormEvent) => {\n    e.preventDefault();\n    if (!inputValue.trim() || isLoading) return;\n\n    // Add user message\n    const userMessage: Message = {\n      id: Date.now().toString(),\n      content: inputValue,\n      role: 'user',\n      timestamp: new Date()\n    };\n\n    setMessages(prev => [...prev, userMessage]);\n    setInputValue('');\n    setIsLoading(true);\n\n    try {\n      // Call the backend API\n      const response = await fetch('http://localhost:8000/api/chat/query', {\n        method: 'POST',\n        headers: {\n          'Content-Type': 'application/json',\n        },\n        body: JSON.stringify({ query: inputValue }),\n      });\n\n      if (!response.ok) {\n        throw new Error(`API request failed with status ${response.status}`);\n      }\n\n      const data = await response.json();\n\n      // Add assistant response\n      const assistantMessage: Message = {\n        id: (Date.now() + 1).toString(),\n        content: data.response,\n        role: 'assistant',\n        timestamp: new Date()\n      };\n\n      setMessages(prev => [...prev, assistantMessage]);\n    } catch (error) {\n      console.error('Error in chatbot:', error);\n      const errorMessage: Message = {\n        id: (Date.now() + 1).toString(),\n        content: 'Sorry, I encountered an error processing your request. Please try again.',\n        role: 'assistant',\n        timestamp: new Date()\n      };\n      setMessages(prev => [...prev, errorMessage]);\n    } finally {\n      setIsLoading(false);\n    }\n  };\n\n  return (\n    <div className=\"chatbot-container\">\n      <div className=\"chatbot-header\">\n        <h3>Physical AI Assistant</h3>\n      </div>\n      \n      <div className=\"chatbot-messages\">\n        {messages.map((message) => (\n          <div \n            key={message.id} \n            className={`message ${message.role}`}\n          >\n            <div className=\"message-content\">\n              {message.content}\n            </div>\n            <div className=\"message-timestamp\">\n              {message.timestamp.toLocaleTimeString([], { hour: '2-digit', minute: '2-digit' })}\n            </div>\n          </div>\n        ))}\n        {isLoading && (\n          <div className=\"message assistant\">\n            <div className=\"message-content\">\n              <div className=\"typing-indicator\">\n                <span></span>\n                <span></span>\n                <span></span>\n              </div>\n            </div>\n          </div>\n        )}\n        <div ref={messagesEndRef} />\n      </div>\n      \n      <form onSubmit={handleSubmit} className=\"chatbot-input-form\">\n        <input\n          type=\"text\"\n          value={inputValue}\n          onChange={(e) => setInputValue(e.target.value)}\n          placeholder=\"Ask about Physical AI, Robotics, ROS, etc...\"\n          className=\"chatbot-input\"\n          disabled={isLoading}\n        />\n        <button \n          type=\"submit\" \n          disabled={!inputValue.trim() || isLoading}\n          className=\"chatbot-submit-btn\"\n        >\n          Send\n        </button>\n      </form>\n    </div>\n  );\n};\n\nconst ChatbotWithSSR: React.FC = () => {\n  return (\n    <BrowserOnly fallback={<div>Loading chatbot...</div>}>\n      {() => <Chatbot />}\n    </BrowserOnly>\n  );\n};\n\nexport default ChatbotWithSSR;\n"})}),"\n",(0,s.jsx)(e.h2,{id:"testing-the-complete-system",children:"Testing the Complete System"}),"\n",(0,s.jsx)(e.h3,{id:"1-local-development-setup",children:"1. Local Development Setup"}),"\n",(0,s.jsx)(e.p,{children:"First, start the backend server:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"cd backend\npip install -r requirements.txt\npython src/api/main.py\n"})}),"\n",(0,s.jsx)(e.p,{children:"Then, in a separate terminal, start the Docusaurus frontend:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-bash",children:"cd docusaurus\nnpm install\nnpm run start\n"})}),"\n",(0,s.jsx)(e.h3,{id:"2-integration-testing",children:"2. Integration Testing"}),"\n",(0,s.jsx)(e.p,{children:"We need to ensure all components work together:"}),"\n",(0,s.jsx)(e.h4,{id:"21-textbook-content-testing",children:"2.1 Textbook Content Testing"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Verify all 6 chapters are displayed in the sidebar"}),"\n",(0,s.jsx)(e.li,{children:"Test navigation between chapters"}),"\n",(0,s.jsx)(e.li,{children:"Ensure content renders correctly"}),"\n"]}),"\n",(0,s.jsx)(e.h4,{id:"22-rag-system-testing",children:"2.2 RAG System Testing"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Ask questions related to textbook content"}),"\n",(0,s.jsx)(e.li,{children:"Verify responses are based on textbook information"}),"\n",(0,s.jsx)(e.li,{children:"Test different types of queries"}),"\n"]}),"\n",(0,s.jsx)(e.h4,{id:"23-simulation-integration-testing",children:"2.3 Simulation Integration Testing"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Verify camera feeds are accessible"}),"\n",(0,s.jsx)(e.li,{children:"Test command execution in simulation"}),"\n",(0,s.jsx)(e.li,{children:"Validate object detection and interaction"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"deployment-considerations",children:"Deployment Considerations"}),"\n",(0,s.jsx)(e.h3,{id:"1-frontend-deployment-vercel",children:"1. Frontend Deployment (Vercel)"}),"\n",(0,s.jsx)(e.p,{children:"For the Docusaurus frontend, we'll need to create a vercel.json configuration:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-json",children:'{\n  "version": 2,\n  "builds": [\n    {\n      "src": "package.json",\n      "use": "@vercel/static-build",\n      "config": {\n        "distDir": "build"\n      }\n    }\n  ],\n  "routes": [\n    {\n      "src": "/(.*)",\n      "dest": "/index.html"\n    }\n  ]\n}\n'})}),"\n",(0,s.jsx)(e.h3,{id:"2-backend-deployment-railwayrender",children:"2. Backend Deployment (Railway/Render)"}),"\n",(0,s.jsx)(e.p,{children:"The backend can be deployed to platforms like Railway or Render. A Dockerfile would be needed:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-dockerfile",children:'FROM python:3.11-slim\n\nWORKDIR /app\n\nCOPY backend/requirements.txt .\nRUN pip install -r requirements.txt\n\nCOPY backend/ .\n\nEXPOSE 8000\n\nCMD ["uvicorn", "src.api.main:app", "--host", "0.0.0.0", "--port", "8000"]\n'})}),"\n",(0,s.jsx)(e.h3,{id:"3-environment-configuration",children:"3. Environment Configuration"}),"\n",(0,s.jsx)(e.p,{children:"Create a .env file for production:"}),"\n",(0,s.jsx)(e.pre,{children:(0,s.jsx)(e.code,{className:"language-env",children:"OPENAI_API_KEY=your-production-openai-key\nQDRANT_URL=your-qdrant-instance-url\nDATABASE_URL=your-database-url\n"})}),"\n",(0,s.jsx)(e.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,s.jsx)(e.h3,{id:"1-frontend-optimizations",children:"1. Frontend Optimizations"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Bundle splitting for faster loading"}),"\n",(0,s.jsx)(e.li,{children:"Image optimization and lazy loading"}),"\n",(0,s.jsx)(e.li,{children:"Caching strategies for content"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"2-backend-optimizations",children:"2. Backend Optimizations"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Vector database indexing for faster retrieval"}),"\n",(0,s.jsx)(e.li,{children:"Caching for frequent queries"}),"\n",(0,s.jsx)(e.li,{children:"Efficient embedding generation"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"3-simulation-optimizations",children:"3. Simulation Optimizations"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Level of detail management"}),"\n",(0,s.jsx)(e.li,{children:"Efficient physics simulation parameters"}),"\n",(0,s.jsx)(e.li,{children:"Optimized rendering settings"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"security-considerations",children:"Security Considerations"}),"\n",(0,s.jsx)(e.h3,{id:"1-api-security",children:"1. API Security"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Rate limiting to prevent abuse"}),"\n",(0,s.jsx)(e.li,{children:"Input validation for all endpoints"}),"\n",(0,s.jsx)(e.li,{children:"Authentication for sensitive operations"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"2-data-security",children:"2. Data Security"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Secure handling of API keys"}),"\n",(0,s.jsx)(e.li,{children:"Privacy considerations for user data"}),"\n",(0,s.jsx)(e.li,{children:"Secure communication protocols"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"future-enhancements",children:"Future Enhancements"}),"\n",(0,s.jsx)(e.h3,{id:"1-advanced-ai-capabilities",children:"1. Advanced AI Capabilities"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"Reinforcement learning integration"}),"\n",(0,s.jsx)(e.li,{children:"Multimodal learning approaches"}),"\n",(0,s.jsx)(e.li,{children:"Improved natural language understanding"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"2-extended-simulation-environments",children:"2. Extended Simulation Environments"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"More complex scenarios and challenges"}),"\n",(0,s.jsx)(e.li,{children:"Integration with real-world data"}),"\n",(0,s.jsx)(e.li,{children:"Multi-robot simulation capabilities"}),"\n"]}),"\n",(0,s.jsx)(e.h3,{id:"3-enhanced-user-interfaces",children:"3. Enhanced User Interfaces"}),"\n",(0,s.jsxs)(e.ul,{children:["\n",(0,s.jsx)(e.li,{children:"VR/AR interfaces for immersive interaction"}),"\n",(0,s.jsx)(e.li,{children:"Voice command capabilities"}),"\n",(0,s.jsx)(e.li,{children:"Gesture recognition interfaces"}),"\n"]}),"\n",(0,s.jsx)(e.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,s.jsx)(e.p,{children:"The Physical AI system we've designed integrates multiple complex technologies to create an interactive humanoid assistant. This project demonstrates the synthesis of:"}),"\n",(0,s.jsxs)(e.ol,{children:["\n",(0,s.jsx)(e.li,{children:"Physical AI principles for embodied intelligence"}),"\n",(0,s.jsx)(e.li,{children:"Humanoid robotics for human-like interaction"}),"\n",(0,s.jsx)(e.li,{children:"ROS 2 for standardized robotics infrastructure"}),"\n",(0,s.jsx)(e.li,{children:"Simulation technology for safe testing"}),"\n",(0,s.jsx)(e.li,{children:"Vision-Language-Action systems for natural interaction"}),"\n"]}),"\n",(0,s.jsx)(e.p,{children:"The system provides a foundation that can be extended with additional capabilities and deployed in various real-world scenarios. By following the architecture and implementation patterns outlined in this capstone, developers can create sophisticated Physical AI systems that advance the field of robotics and human-robot interaction."}),"\n",(0,s.jsx)(e.p,{children:"This concludes our comprehensive exploration of Physical AI and Humanoid Robotics. Through these chapters, we've covered the fundamental concepts, essential technologies, and practical implementation approaches needed to build intelligent, embodied systems."}),"\n",(0,s.jsx)(e.p,{children:"The next step for readers is to experiment with the code examples, extend the system with additional capabilities, and contribute to the growing field of Physical AI."})]})}function m(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,s.jsx)(e,{...n,children:(0,s.jsx)(d,{...n})}):d(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>o,x:()=>r});var i=t(6540);const s={},a=i.createContext(s);function o(n){const e=i.useContext(a);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(s):n.components||s:o(n.components),i.createElement(a.Provider,{value:e},n.children)}}}]);